{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdca220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import requests\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SwinIR.network_swinir import SwinIR as net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1756fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SwinIR.util_calculate_psnr_ssim import calculate_psnr, calculate_ssim, bgr2ycbcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c1574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "'001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth', # training patch 64 and scale 2\n",
    "'001_classicalSR_DF2K_s64w8_SwinIR-M_x3.pth',\n",
    "'001_classicalSR_DF2K_s64w8_SwinIR-M_x4.pth',\n",
    "'001_classicalSR_DF2K_s64w8_SwinIR-M_x8.pth', # training patch 64 and scale 8\n",
    "'001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth',# training patch 48 and scale 2\n",
    "'001_classicalSR_DIV2K_s48w8_SwinIR-M_x3.pth',\n",
    "'001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth',\n",
    "'001_classicalSR_DIV2K_s48w8_SwinIR-M_x8.pth',# training patch 48 and scale 8\n",
    "'002_lightweightSR_DIV2K_s64w8_SwinIR-S_x2.pth',# training patch 64 and scale 2\n",
    "'002_lightweightSR_DIV2K_s64w8_SwinIR-S_x3.pth',\n",
    "'002_lightweightSR_DIV2K_s64w8_SwinIR-S_x4.pth',# training patch 64 and scale 4\n",
    "'003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN-with-dict-keys-params-and-params_ema.pth',\n",
    "'003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth',\n",
    "'003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_PSNR-with-dict-keys-params-and-params_ema.pth',\n",
    "'003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_PSNR.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN-with-dict-keys-params-and-params_ema.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_PSNR-with-dict-keys-params-and-params_ema.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_PSNR.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN-with-dict-keys-params-and-params_ema.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_PSNR-with-dict-keys-params-and-params_ema.pth',\n",
    "'003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_PSNR.pth',\n",
    "'004_grayDN_DFWB_s128w8_SwinIR-M_noise15.pth',\n",
    "'004_grayDN_DFWB_s128w8_SwinIR-M_noise25.pth',\n",
    "'004_grayDN_DFWB_s128w8_SwinIR-M_noise50.pth',\n",
    "'005_colorDN_DFWB_s128w8_SwinIR-M_noise15.pth',\n",
    "'005_colorDN_DFWB_s128w8_SwinIR-M_noise25.pth',\n",
    "'005_colorDN_DFWB_s128w8_SwinIR-M_noise50.pth',\n",
    "'006_CAR_DFWB_s126w7_SwinIR-M_jpeg10.pth',\n",
    "'006_CAR_DFWB_s126w7_SwinIR-M_jpeg20.pth',\n",
    "'006_CAR_DFWB_s126w7_SwinIR-M_jpeg30.pth',\n",
    "'006_CAR_DFWB_s126w7_SwinIR-M_jpeg40.pth',\n",
    "'006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg10.pth',\n",
    "'006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg20.pth',\n",
    "'006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg30.pth',\n",
    "'006_colorCAR_DFWB_s126w7_SwinIR-M_jpeg40.pth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "963c50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(model_name):    \n",
    "    model_path = '/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/models/'\n",
    "    if os.path.exists(model_path + model_name):\n",
    "        print(f'Model {model_name} already exists at {model_path}.')\n",
    "    else:\n",
    "        print(f'Model {model_name} not found. Downloading...')\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        url = 'https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/{}'.format(model_name)\n",
    "        print(f'downloading model {url}')\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(model_path+model_name, 'wb').write(r.content)\n",
    "        print(f'Downloaded model {model_name} to {model_path}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4054c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth not found. Downloading...\n",
      "downloading model https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth\n",
      "Downloaded model 001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth to /Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/models/\n"
     ]
    }
   ],
   "source": [
    "download_model(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8201a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(task, scale, training_patch, large_model, model_name) -> net:\n",
    "    model_path = '/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/models/' + model_name\n",
    "    # 001 classical image sr\n",
    "    if task == 'classical_sr':\n",
    "        model = net(upscale=scale, in_chans=3, img_size=training_patch, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='pixelshuffle', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 002 lightweight image sr\n",
    "    # use 'pixelshuffledirect' to save parameters\n",
    "    elif task == 'lightweight_sr':\n",
    "        model = net(upscale=scale, in_chans=3, img_size=64, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6], embed_dim=60, num_heads=[6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='pixelshuffledirect', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 003 real-world image sr\n",
    "    elif task == 'real_sr':\n",
    "        if not large_model:\n",
    "            # use 'nearest+conv' to avoid block artifacts\n",
    "            model = net(upscale=scale, in_chans=3, img_size=64, window_size=8,\n",
    "                        img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='1conv')\n",
    "        else:\n",
    "            # larger model size; use '3conv' to save parameters and memory; use ema for GAN training\n",
    "            model = net(upscale=scale, in_chans=3, img_size=64, window_size=8,\n",
    "                        img_range=1., depths=[6, 6, 6, 6, 6, 6, 6, 6, 6], embed_dim=240,\n",
    "                        num_heads=[8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
    "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='3conv')\n",
    "        param_key_g = 'params_ema'\n",
    "\n",
    "    # 004 grayscale image denoising\n",
    "    elif task == 'gray_dn':\n",
    "        model = net(upscale=1, in_chans=1, img_size=128, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 005 color image denoising\n",
    "    elif task == 'color_dn':\n",
    "        model = net(upscale=1, in_chans=3, img_size=128, window_size=8,\n",
    "                    img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 006 grayscale JPEG compression artifact reduction\n",
    "    # use window_size=7 because JPEG encoding uses 8x8; use img_range=255 because it's sligtly better than 1\n",
    "    elif task == 'jpeg_car':\n",
    "        model = net(upscale=1, in_chans=1, img_size=126, window_size=7,\n",
    "                    img_range=255., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    # 006 color JPEG compression artifact reduction\n",
    "    # use window_size=7 because JPEG encoding uses 8x8; use img_range=255 because it's sligtly better than 1\n",
    "    elif task == 'color_jpeg_car':\n",
    "        model = net(upscale=1, in_chans=3, img_size=126, window_size=7,\n",
    "                    img_range=255., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "                    mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "        param_key_g = 'params'\n",
    "\n",
    "    pretrained_model = torch.load(model_path)\n",
    "    model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9c93981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520 368 195.589056\n",
      "Tensor shape: torch.Size([1, 3, 520, 368])\n",
      "Output shape: torch.Size([1, 3, 1040, 736])\n"
     ]
    }
   ],
   "source": [
    "upscale = 2\n",
    "window_size = 8\n",
    "height = (1024 // upscale // window_size + 1) * window_size\n",
    "width = (720 // upscale // window_size + 1) * window_size\n",
    "model = net(upscale=2, img_size=(height, width),\n",
    "                   window_size=window_size, img_range=1., depths=[6, 6, 6, 6],\n",
    "                   embed_dim=60, num_heads=[6, 6, 6, 6], mlp_ratio=2, upsampler='pixelshuffledirect')\n",
    "#print(model)\n",
    "print(height, width, model.flops() / 1e9)\n",
    "\n",
    "x = torch.randn((1, 3, height, width))\n",
    "print(f\"Tensor shape: {x.shape}\")\n",
    "x = model(x)\n",
    "print(f\"Output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "683ab1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_folder(task, scale, folder_gt, large_model=False):\n",
    "    save_dir = f\"/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/images/{task}/x{scale}\"\n",
    "    folder = folder_gt\n",
    "    if task in ['classical_sr', 'lightweight_sr']:  \n",
    "        border = scale\n",
    "        window_size = 8\n",
    "    \n",
    "    elif task in ['real_sr']:\n",
    "        if large_model:\n",
    "            save_dir += '_large'\n",
    "        border = 0\n",
    "        window_size = 8\n",
    "    \n",
    "    elif task in ['gray_dn', 'color_dn']:\n",
    "        border = 0\n",
    "        window_size = 8\n",
    "        \n",
    "    elif task in ['jpeg_car', 'color_jpeg_car']:\n",
    "        border = 0\n",
    "        window_size = 7\n",
    "        \n",
    "    return folder, save_dir, border, window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69346532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pair(task, path, folder_lq, scale, noise=15, jpeg=40):\n",
    "    (imgname, imgext) = os.path.splitext(os.path.basename(path))\n",
    "    \n",
    "    # 001 classical image sr/ 002 lightweight image sr (load lq-gt image pairs)\n",
    "    if task in ['classical_sr', 'lightweight_sr']:\n",
    "        img_gt = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "        img_lq = cv2.imread(f'{folder_lq}/{imgname}x{scale}{imgext}', cv2.IMREAD_COLOR).astype(\n",
    "            np.float32) / 255.\n",
    "\n",
    "    # 003 real-world image sr (load lq image only)\n",
    "    elif task in ['real_sr']:\n",
    "        img_gt = None\n",
    "        img_lq = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "\n",
    "    # 004 grayscale image denoising (load gt image and generate lq image on-the-fly)\n",
    "    elif task in ['gray_dn']:\n",
    "        img_gt = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.\n",
    "        np.random.seed(seed=0)\n",
    "        img_lq = img_gt + np.random.normal(0, noise / 255., img_gt.shape)\n",
    "        img_gt = np.expand_dims(img_gt, axis=2)\n",
    "        img_lq = np.expand_dims(img_lq, axis=2)\n",
    "\n",
    "    # 005 color image denoising (load gt image and generate lq image on-the-fly)\n",
    "    elif task in ['color_dn']:\n",
    "        img_gt = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "        np.random.seed(seed=0)\n",
    "        img_lq = img_gt + np.random.normal(0, noise / 255., img_gt.shape)\n",
    "\n",
    "    # 006 grayscale JPEG compression artifact reduction (load gt image and generate lq image on-the-fly)\n",
    "    elif task in ['jpeg_car']:\n",
    "        img_gt = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        if img_gt.ndim != 2:\n",
    "            img_gt = bgr2ycbcr(img_gt, y_only=True)\n",
    "        result, encimg = cv2.imencode('.jpg', img_gt, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg])\n",
    "        img_lq = cv2.imdecode(encimg, 0)\n",
    "        img_gt = np.expand_dims(img_gt, axis=2).astype(np.float32) / 255.\n",
    "        img_lq = np.expand_dims(img_lq, axis=2).astype(np.float32) / 255.\n",
    "\n",
    "    # 006 JPEG compression artifact reduction (load gt image and generate lq image on-the-fly)\n",
    "    elif task in ['color_jpeg_car']:\n",
    "        img_gt = cv2.imread(path)\n",
    "        result, encimg = cv2.imencode('.jpg', img_gt, [int(cv2.IMWRITE_JPEG_QUALITY), jpeg])\n",
    "        img_lq = cv2.imdecode(encimg, 1)\n",
    "        img_gt = img_gt.astype(np.float32)/ 255.\n",
    "        img_lq = img_lq.astype(np.float32)/ 255.\n",
    "\n",
    "    return imgname, img_lq, img_gt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "014da63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(img_lq, model, args, window_size):\n",
    "    if args.tile is None:\n",
    "        # test the image as a whole\n",
    "        output = model(img_lq)\n",
    "    else:\n",
    "        # test the image tile by tile\n",
    "        b, c, h, w = img_lq.size()\n",
    "        tile = min(args.tile, h, w)\n",
    "        assert tile % window_size == 0, \"tile size should be a multiple of window_size\"\n",
    "        tile_overlap = args.tile_overlap\n",
    "        sf = args.scale\n",
    "\n",
    "        stride = tile - tile_overlap\n",
    "        h_idx_list = list(range(0, h-tile, stride)) + [h-tile]\n",
    "        w_idx_list = list(range(0, w-tile, stride)) + [w-tile]\n",
    "        E = torch.zeros(b, c, h*sf, w*sf).type_as(img_lq)\n",
    "        W = torch.zeros_like(E)\n",
    "\n",
    "        for h_idx in h_idx_list:\n",
    "            for w_idx in w_idx_list:\n",
    "                in_patch = img_lq[..., h_idx:h_idx+tile, w_idx:w_idx+tile]\n",
    "                out_patch = model(in_patch)\n",
    "                out_patch_mask = torch.ones_like(out_patch)\n",
    "\n",
    "                E[..., h_idx*sf:(h_idx+tile)*sf, w_idx*sf:(w_idx+tile)*sf].add_(out_patch)\n",
    "                W[..., h_idx*sf:(h_idx+tile)*sf, w_idx*sf:(w_idx+tile)*sf].add_(out_patch_mask)\n",
    "        output = E.div_(W)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c43624d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "306e03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model('classical_sr', \n",
    "                     2, \n",
    "                     64, \n",
    "                     False, \n",
    "                     models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1aca2795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinIR(\n",
       "  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (patch_unembed): PatchUnEmbed()\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=180, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.003)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.006)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.011)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.014)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (1): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=180, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.020)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.023)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.029)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.031)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (2): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=180, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.034)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.037)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.040)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.046)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.049)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (3): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=180, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.051)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.054)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.060)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.063)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.066)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (4): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=180, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.069)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.071)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.077)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.080)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (5): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=180, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.086)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.089)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.094)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.097)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=180, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=180, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_before_upsample): Sequential(\n",
       "    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       "  (upsample): Upsample(\n",
       "    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): PixelShuffle(upscale_factor=2)\n",
       "  )\n",
       "  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2ef68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95b86594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the input image\n",
    "input_path = 'SwinIR/images/X2/butterflyx2.png'\n",
    "img = cv2.imread(input_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "img = np.transpose(img if img.shape[2] == 1 else img[:, :, [2, 1, 0]], (2, 0, 1))\n",
    "\n",
    "# Convert to tensor and add batch dimension\n",
    "img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "window_size = 8  # Assuming window size is 8 for the model\n",
    "# Run through model\n",
    "with torch.no_grad():   \n",
    "    _, _, h_old, w_old = img_tensor.shape\n",
    "    h_pad = (h_old // window_size + 1) * window_size - h_old\n",
    "    w_pad = (w_old // window_size + 1) * window_size - w_old\n",
    "    image = torch.cat([img_tensor, torch.flip(img_tensor, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "    image = torch.cat([image, torch.flip(image, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "            \n",
    "    output = model(image)\n",
    "    output = output[:, :, :h_old * 2, :w_old * 2]  # Crop to original size\n",
    "\n",
    "# Convert output tensor to image\n",
    "output_img = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "if output_img.ndim == 3:\n",
    "    output_img = np.transpose(output_img[[2,1,0], :, :], (1,2,0) )   \n",
    "output_img = (output_img * 255.0).round().astype(np.uint8)\n",
    "\n",
    "# Save the output image\n",
    "cv2.imwrite('SwinIR/images/X2/butterflyx2_output.png', output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6660ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_psnr_ssim(img_1, img_2):\n",
    "    \"\"\"\n",
    "    Evaluate PSNR and SSIM between two images.\n",
    "    \n",
    "    Args:\n",
    "        img_1 (numpy.ndarray): First image.\n",
    "        img_2 (numpy.ndarray): Second image.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: PSNR and SSIM values.\n",
    "    \"\"\"\n",
    "    psnr = calculate_psnr(img_1, img_2, crop_border=8)\n",
    "    ssim = calculate_ssim(img_1, img_2, crop_border=8)\n",
    "    return psnr, ssim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ebc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = output_img\n",
    "img_2 = cv2.imread('/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/images/HR/butterfly.png', cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "img_gt = (img_2 * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
    "img_gt = img_gt[:h_old * 2, :w_old * 2, ...]  # crop gt\n",
    "img_gt = np.squeeze(img_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e015173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 34.70, SSIM: 0.9739\n"
     ]
    }
   ],
   "source": [
    "eval_psnr, eval_ssim = evaluate_image_psnr_ssim(img_1, img_gt)\n",
    "print(f\"PSNR: {eval_psnr:.2f}, SSIM: {eval_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder, save_dir, border, window_size = setup_folder(\n",
    "    'classical_sr', \n",
    "    2, \n",
    "    '/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/images/classical_sr/x2', large_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1930c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fine-tuning the SwinIR model on a single image (img_tensor, img_gt)\n",
    "# Note: This is a minimal example for demonstration. For real fine-tuning, use a dataset and DataLoader.\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prepare data\n",
    "input_tensor = img_tensor  # shape: (1, 3, H, W)\n",
    "target = torch.from_numpy(np.transpose(img_gt, (2, 0, 1))).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "# Set model to train mode\n",
    "model.train()\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Fine-tune for a few steps\n",
    "num_steps = 5\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step+1}/{num_steps}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Set back to eval mode after fine-tuning\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209487ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input images: 10\n",
      "Number of target images: 5\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset: X2 folder as input (low-resolution), HR folder as target (high-resolution)\n",
    "input_folder = '/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/images/X2'\n",
    "target_folder = '/Volumes/TOSHIBA/Github Repositories/transformers/SwinIR/images/HR'\n",
    "\n",
    "input_images = sorted(glob.glob(os.path.join(input_folder, '*.png')))\n",
    "target_images = sorted(glob.glob(os.path.join(target_folder, '*.png')))\n",
    "\n",
    "print(f\"Number of input images: {len(input_images)}\")\n",
    "print(f\"Number of target images: {len(target_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Prepare data\n",
    "input_tensor = img_tensor  # shape: (1, 3, H, W)\n",
    "target = torch.from_numpy(np.transpose(img_gt, (2, 0, 1))).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "# Set model to train mode\n",
    "model.train()\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Fine-tune for a few steps\n",
    "num_steps = 5\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step+1}/{num_steps}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Set back to eval mode after fine-tuning\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4799ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, input_paths, target_paths, transform=None):\n",
    "        self.input_paths = input_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load input (low-res) image\n",
    "        img_input = cv2.imread(self.input_paths[idx], cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "        img_input = np.transpose(img_input[:, :, [2, 1, 0]], (2, 0, 1))  # BGR to RGB, HWC to CHW\n",
    "\n",
    "        # Load target (high-res) image\n",
    "        img_target = cv2.imread(self.target_paths[idx], cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "        img_target = np.transpose(img_target[:, :, [2, 1, 0]], (2, 0, 1))\n",
    "\n",
    "        input_tensor = torch.from_numpy(img_input).float()\n",
    "        target_tensor = torch.from_numpy(img_target).float()\n",
    "\n",
    "        if self.transform:\n",
    "            input_tensor, target_tensor = self.transform(input_tensor, target_tensor)\n",
    "\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ImagePairDataset(input_images, target_images)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model using the dataloader with batch size 2\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_steps = 5\n",
    "step = 0\n",
    "for input_batch, target_batch in dataloader:\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    step += 1\n",
    "    if step >= num_steps:\n",
    "        break\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff53d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model on the dataset\n",
    "model.eval()\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "\n",
    "for inp_path, tgt_path in zip(input_images, target_images):\n",
    "    # Load input and target images\n",
    "    inp_img = cv2.imread(inp_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "    tgt_img = cv2.imread(tgt_path, cv2.IMREAD_COLOR).astype(np.float32) / 255.0\n",
    "\n",
    "    # Prepare input tensor\n",
    "    inp_tensor = np.transpose(inp_img[:, :, [2, 1, 0]], (2, 0, 1))\n",
    "    inp_tensor = torch.from_numpy(inp_tensor).unsqueeze(0).to(device)\n",
    "\n",
    "    # Pad input if needed\n",
    "    _, _, h_old, w_old = inp_tensor.shape\n",
    "    h_pad = (h_old // 8 + 1) * 8 - h_old\n",
    "    w_pad = (w_old // 8 + 1) * 8 - w_old\n",
    "    image = torch.cat([inp_tensor, torch.flip(inp_tensor, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "    image = torch.cat([image, torch.flip(image, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        output = output[:, :, :h_old * 2, :w_old * 2]\n",
    "\n",
    "    # Convert output tensor to image\n",
    "    output_img = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "    if output_img.ndim == 3:\n",
    "        output_img = np.transpose(output_img[[2, 1, 0], :, :], (1, 2, 0))\n",
    "    output_img = (output_img * 255.0).round().astype(np.uint8)\n",
    "\n",
    "    # Prepare ground truth\n",
    "    tgt_img_uint8 = (tgt_img * 255.0).round().astype(np.uint8)\n",
    "    tgt_img_uint8 = tgt_img_uint8[:h_old * 2, :w_old * 2, ...]\n",
    "    tgt_img_uint8 = np.squeeze(tgt_img_uint8)\n",
    "\n",
    "    # Evaluate\n",
    "    psnr, ssim = evaluate_image_psnr_ssim(output_img, tgt_img_uint8)\n",
    "    psnr_list.append(psnr)\n",
    "    ssim_list.append(ssim)\n",
    "\n",
    "print(f\"Average PSNR: {np.mean(psnr_list):.2f}, Average SSIM: {np.mean(ssim_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b4c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317d481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
